<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>answer-site | Camera</title>
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

  <!-- Firebase -->
  <script src="https://www.gstatic.com/firebasejs/11.1.0/firebase-app-compat.js"></script>
  <script src="https://www.gstatic.com/firebasejs/11.1.0/firebase-database-compat.js"></script>

  <!-- Tesseract OCR -->
  <script src="https://unpkg.com/tesseract.js@4/dist/tesseract.min.js"></script>

  <style>
    * { box-sizing: border-box; }
    body {
      margin: 0;
      padding: 6px;
      background: #000;
      color: #fff;
      font-family: system-ui, -apple-system, BlinkMacSystemFont, sans-serif;
    }
    .wrap {
      width: 100%;
    }

    /* ìƒë‹¨: ì¹´ë©”ë¼ ì˜ì—­ */
    #videoContainer {
      position: relative;
      width: 100%;
      max-height: calc(100vh - 120px);
      overflow: hidden;
      background: #000;
      border-radius: 8px;
    }
    #video {
      width: 100%;
      height: auto;
      transform-origin: center center;
    }
    #overlayFrame {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      border: 2px solid rgba(0, 255, 0, 0.4);
      border-radius: 6px;
      pointer-events: none;
    }

    /* í•˜ë‹¨ ì»¨íŠ¸ë¡¤: ìƒíƒœ + ë°°ìœ¨ */
    #controls {
      margin-top: 6px;
      font-size: 11px;
    }

    #statusLine {
      margin-bottom: 4px;
      color: #c4f1ff;
      white-space: nowrap;
      overflow: hidden;
      text-overflow: ellipsis;
    }

    .row {
      display: flex;
      align-items: center;
      gap: 6px;
    }
    .label {
      font-size: 11px;
      color: #aaa;
      white-space: nowrap;
    }
    #zoomRange {
      flex: 1 1 auto;
    }
  </style>
</head>
<body>
<div class="wrap">
  <div id="videoContainer">
    <video id="video" autoplay playsinline></video>
    <div id="overlayFrame"></div>
  </div>

  <div id="controls">
    <div id="statusLine">ì¹´ë©”ë¼ ì¤€ë¹„ ì¤‘...</div>

    <div class="row">
      <span class="label">ë°°ìœ¨</span>
      <input id="zoomRange" type="range" min="1" max="3" step="0.05" value="1">
    </div>
  </div>
</div>

<script>
  // ---------------- Firebase ----------------
  const firebaseConfig = {
    apiKey: "AIzaSyAvjpHfmbuHQq3ZeV6mNKQFI9LsnX-vf68",
    authDomain: "answer-site-p2p.firebaseapp.com",
    databaseURL: "https://answer-site-p2p-default-rtdb.asia-southeast1.firebasedatabase.app",
    projectId: "answer-site-p2p",
    storageBucket: "answer-site-p2p.firebasestorage.app",
    messagingSenderId: "364227113735",
    appId: "1:364227113735:web:b18355cf0b16454663cba2",
    measurementId: "G-C8MRHK5ZGS"
  };
  firebase.initializeApp(firebaseConfig);
  const db = firebase.database();

  const video      = document.getElementById('video');
  const statusLine = document.getElementById('statusLine');
  const zoomRange  = document.getElementById('zoomRange');

  // ğŸ”¤ ì˜ì–´ ì „ìš© OCR
  const OCR_LANG = "eng";

  let currentMode = "reading";
  db.ref("p2p/state/modeHint").on("value", snap => {
    const m = snap.val();
    if (m) {
      currentMode = m;
      updateStatusLine();
    }
  });

  let ocrRunning = false;
  let lastSolvedSnippet = "";
  let lastBlocked = null;
  let lastOcrTime = 0;

  // ë°ê¸° + ì›€ì§ì„ ê°ì§€ìš© ì¸ë„¤ì¼ ìº”ë²„ìŠ¤
  const BW = 64, BH = 36;
  const bCanvas = document.createElement('canvas');
  const bCtx = bCanvas.getContext('2d');
  bCanvas.width = BW;
  bCanvas.height = BH;
  let prevThumb = null;
  let lastStableTime = 0;

  // ê³ í•´ìƒë„ ìŠ¤í‹¸ í”„ë ˆì„ìš© ImageCapture
  let imageCapture = null;

  // ì„¼ì„œ ì¤Œ ê´€ë ¨
  let zoomSupport = false;
  let zoomTrack = null;
  let zoomMin = 1;
  let zoomMax = 3;
  let zoomStep = 0.1;

  function updateStatusLine(extra) {
    const base = `ì¹´ë©”ë¼ ON â€“ ëª¨ë“œ: ${currentMode} (í™”ë©´ ì•ˆì •ë˜ë©´ ìë™ ì¸ì‹${zoomSupport ? ", ì„¼ì„œ ì¤Œ" : ""})`;
    statusLine.textContent = extra || base;
  }

  // ---------- ì˜ì–´ë§Œ ë‚¨ê¸°ê¸° ----------
  function extractEnglishText(raw) {
    if (!raw) return "";
    const lines = raw.split(/\r?\n/);
    const cleaned = lines.map(line =>
      line
        .replace(/[^A-Za-z0-9.,;:?!'"()\- \t]/g, ' ')  // ì˜ì–´/ìˆ«ì/ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸ ì™¸ ì œê±°
        .replace(/\s+/g, ' ')
        .trim()
    ).filter(line => line.length > 0);
    return cleaned.join('\n');
  }

  // ---------- í•˜ì´ë¼ì´íŠ¸/í˜•ê´‘íœ ëŒ€ë¹„ ë³´ì • ----------
  function preprocessCanvasForText(canvas, ctx) {
    const imgData = ctx.getImageData(0, 0, canvas.width, canvas.height);
    const data = imgData.data;
    for (let i = 0; i < data.length; i += 4) {
      const r = data[i];
      const g = data[i + 1];
      const b = data[i + 2];
      const gray = (r + g + b) / 3;

      let v;
      if (gray > 210) {
        v = 255;      // ì•„ì£¼ ë°ì€ ë¶€ë¶„(í˜•ê´‘íœ í¬í•¨)ì€ í°ìƒ‰
      } else if (gray < 40) {
        v = 0;        // ì•„ì£¼ ì–´ë‘ìš´ ë¶€ë¶„ì€ ê²€ì •
      } else {
        v = gray;
      }
      data[i] = data[i + 1] = data[i + 2] = v;
    }
    ctx.putImageData(imgData, 0, 0);
  }

  // ---------- ì¹´ë©”ë¼ ----------
  async function startCamera() {
    try {
      // 1ì°¨ ì‹œë„: ê³ í•´ìƒë„ ìŠ¤íŠ¸ë¦¼ ìš”ì²­ (ê°€ëŠ¥í•˜ë©´ 4K ê·¼ì²˜)
      const hiConstraints = {
        video: {
          facingMode: { ideal: "environment" },
          width:  { ideal: 3840, max: 4096 },
          height: { ideal: 2160, max: 4096 },
          frameRate: { ideal: 30 }
        },
        audio: false
      };

      const fallbackConstraints = {
        video: {
          facingMode: { ideal: "environment" },
          width:  { min: 1280, ideal: 1920, max: 2560 },
          height: { min: 720,  ideal: 1080, max: 1440 }
        },
        audio: false
      };

      let stream;
      try {
        stream = await navigator.mediaDevices.getUserMedia(hiConstraints);
      } catch (e) {
        console.warn("ê³ í•´ìƒë„ ìŠ¤íŠ¸ë¦¼ ì‹¤íŒ¨, fallback ì‚¬ìš©:", e);
        stream = await navigator.mediaDevices.getUserMedia(fallbackConstraints);
      }

      video.srcObject = stream;

      const track = stream.getVideoTracks()[0];
      setupZoom(track);

      if ('ImageCapture' in window && track) {
        try {
          imageCapture = new ImageCapture(track);
        } catch (e) {
          console.warn("ImageCapture ìƒì„± ì‹¤íŒ¨, video í”„ë ˆì„ fallback:", e);
          imageCapture = null;
        }
      }

      updateStatusLine();
      startAutoLoop();
    } catch (e) {
      console.error(e);
      statusLine.textContent = "ì¹´ë©”ë¼ ì ‘ê·¼ ì‹¤íŒ¨: " + e.toString();
    }
  }

  // ---------- ì„¼ì„œ ì¤Œ ì´ˆê¸°í™” ----------
  function setupZoom(track) {
    zoomTrack = track;
    zoomSupport = false;

    try {
      const caps = track.getCapabilities ? track.getCapabilities() : {};
      if (caps && typeof caps.zoom === 'object') {
        zoomSupport = true;
        zoomMin = (typeof caps.zoom.min === 'number') ? caps.zoom.min : 1;
        zoomMax = (typeof caps.zoom.max === 'number') ? caps.zoom.max : 3;
        zoomStep = (typeof caps.zoom.step === 'number' && caps.zoom.step > 0) ? caps.zoom.step : 0.1;

        if (zoomMax > 5) zoomMax = 5;
        if (zoomMin < 1) zoomMin = 1;

        zoomRange.min = zoomMin.toString();
        zoomRange.max = zoomMax.toString();
        zoomRange.step = zoomStep.toString();

        // ì‹œì‘í•  ë•Œ ì ë‹¹íˆ í¬ê²Œ (ìµœëŒ€ 3ë°°ê¹Œì§€)
        const targetZoom = Math.min(3, zoomMax);
        zoomRange.value = targetZoom.toString();
        applyZoomToTrack(targetZoom);
        video.style.transform = "scale(1)";

        console.log('Zoom capability detected:', caps.zoom);
      } else {
        console.log('Zoom capability not found, fallback to CSS scale.');
        zoomSupport = false;
        zoomRange.min = "1";
        zoomRange.max = "3";
        zoomRange.step = "0.05";
        zoomRange.value = "1.8";
        video.style.transform = "scale(1.8)"; // ë¯¸ì§€ì› ê¸°ê¸°ëŠ” í™”ë©´ í™•ëŒ€ë¼ë„
      }
    } catch (e) {
      console.warn('setupZoom error, fallback to CSS scale:', e);
      zoomSupport = false;
      zoomRange.min = "1";
      zoomRange.max = "3";
      zoomRange.step = "0.05";
      zoomRange.value = "1.8";
      video.style.transform = "scale(1.8)";
    }
  }

  function applyZoomToTrack(z) {
    if (!zoomTrack || !zoomSupport) return;
    zoomTrack.applyConstraints({
      advanced: [{ zoom: z }]
    }).catch(err => {
      console.warn('ì„¼ì„œ ì¤Œ ì ìš© ì‹¤íŒ¨, CSS í™•ëŒ€ë§Œ ì‚¬ìš©:', err);
      zoomSupport = false;
    });
  }

  zoomRange.addEventListener("input", () => {
    const val = parseFloat(zoomRange.value);
    if (zoomSupport) {
      applyZoomToTrack(val);
      video.style.transform = "scale(1)";
    } else {
      video.style.transform = `scale(${val})`;
    }
  });

  // ---------- ë°ê¸° + ì›€ì§ì„ ë¶„ì„ ----------
  function analyzeFrame() {
    if (!video.videoWidth || !video.videoHeight) {
      return { blocked: false, stable: false };
    }

    bCtx.drawImage(video, 0, 0, BW, BH);
    const data = bCtx.getImageData(0, 0, BW, BH).data;
    const len = BW * BH;

    let sumBright = 0;
    let diffSum = 0;
    const currThumb = new Uint8Array(len);

    for (let i = 0, p = 0; i < data.length; i += 4, p++) {
      const r = data[i];
      const g = data[i + 1];
      const b = data[i + 2];
      const gray = (r + g + b) / 3;
      currThumb[p] = gray;
      sumBright += gray;
      if (prevThumb) {
        diffSum += Math.abs(gray - prevThumb[p]);
      }
    }

    const avgBright = sumBright / len;
    const diffAvg   = prevThumb ? (diffSum / len) : 0;

    const blocked = avgBright < 25;

    const now = Date.now();
    let stable = false;
    if (diffAvg < 8) {
      if (!lastStableTime) {
        lastStableTime = now;
      } else if (now - lastStableTime > 1000) {
        stable = true;
      }
    } else {
      lastStableTime = 0;
    }

    prevThumb = currThumb;

    return { blocked, stable };
  }

  // ---------- ìë™ ë£¨í”„ ----------
  function startAutoLoop() {
    function loop() {
      const now = Date.now();

      const { blocked, stable } = analyzeFrame();

      if (blocked !== lastBlocked) {
        lastBlocked = blocked;
        db.ref("p2p/state").update({
          cameraBlocked: blocked,
          cameraUpdatedAt: Date.now()
        });
        statusLine.textContent = blocked
          ? "ì¹´ë©”ë¼ê°€ ê°€ë ¤ì§ â€“ ë§ˆì§€ë§‰ ë‹µ ìœ ì§€ ì¤‘"
          : `ì¹´ë©”ë¼ ON â€“ ëª¨ë“œ: ${currentMode} (í™”ë©´ ì•ˆì •ë˜ë©´ ìë™ ì¸ì‹${zoomSupport ? ", ì„¼ì„œ ì¤Œ" : ""})`;
      }

      // 8ì´ˆë§ˆë‹¤, í™”ë©´ ì•ˆì • + ì•ˆ ê°€ë ¤ì§ + OCR ì•ˆ ëŒê³  ìˆìœ¼ë©´ í•œ ë²ˆ ì¸ì‹
      if (!blocked && stable && !ocrRunning && now - lastOcrTime > 8000) {
        lastOcrTime = now;
        doOCROnce(false);
      }

      requestAnimationFrame(loop);
    }
    requestAnimationFrame(loop);
  }

  // ---------- ê³ í•´ìƒë„ ìŠ¤í‹¸ ìš°ì„  ìº¡ì²˜ ----------
  async function captureToCanvas(canvas, ctx) {
    if (imageCapture && typeof imageCapture.takePhoto === "function") {
      try {
        const blob = await imageCapture.takePhoto();
        const imgBitmap = await createImageBitmap(blob);
        drawCroppedScaled(imgBitmap, canvas, ctx);
        return;
      } catch (e) {
        console.warn("takePhoto ì‹¤íŒ¨, grabFrame ì‹œë„:", e);
      }
    }

    if (imageCapture && typeof imageCapture.grabFrame === "function") {
      try {
        const bitmap = await imageCapture.grabFrame();
        drawCroppedScaled(bitmap, canvas, ctx);
        return;
      } catch (e) {
        console.warn("grabFrame ì‹¤íŒ¨, video í”„ë ˆì„ fallback:", e);
      }
    }

    fillCanvasFromVideoCenter(canvas, ctx);
  }

  // ì¤‘ì•™ í¬ë¡­ + ìŠ¤ì¼€ì¼ + ì „ì²˜ë¦¬
  function drawCroppedScaled(source, canvas, ctx) {
    const fullW = source.width;
    const fullH = source.height;

    const scaleCrop = 0.85;
    const cropW = Math.floor(fullW * scaleCrop);
    const cropH = Math.floor(fullH * scaleCrop);
    const cropX = Math.floor((fullW - cropW) / 2);
    const cropY = Math.floor((fullH - cropH) / 2);

    const maxW = 2000;
    const scale = Math.min(1, maxW / cropW);
    const outW = Math.floor(cropW * scale);
    const outH = Math.floor(cropH * scale);

    canvas.width = outW;
    canvas.height = outH;
    ctx.drawImage(
      source,
      cropX, cropY, cropW, cropH,
      0, 0, outW, outH
    );

    preprocessCanvasForText(canvas, ctx);
  }

  // ---------- OCR ----------
  async function doOCROnce(manual) {
    if (ocrRunning) return;
    ocrRunning = true;

    try {
      statusLine.textContent = manual
        ? "ìˆ˜ë™ OCR ì§„í–‰ ì¤‘..."
        : "ìë™ OCR ì§„í–‰ ì¤‘...";

      const canvas = document.createElement('canvas');
      const ctx = canvas.getContext('2d');

      await captureToCanvas(canvas, ctx);

      const result = await Tesseract.recognize(canvas, OCR_LANG, {
        logger: m => {
          if (m.status === "recognizing text") {
            statusLine.textContent =
              (manual ? "[ìˆ˜ë™] " : "[ìë™] ") +
              `OCR ì§„í–‰ ì¤‘... ${Math.round((m.progress || 0) * 100)}%`;
          }
        }
      });

      const rawText = result.data.text || "";
      const tConf = result.data.confidence || 0;

      // ğŸ”´ OCR ìì²´ confidence 10% ë¯¸ë§Œì´ë©´ ê·¸ëƒ¥ ë²„ë¦¼
      if (tConf < 10) {
        statusLine.textContent =
          `OCR ì‹¤íŒ¨ (confidence=${tConf.toFixed(1)}% < 10%) â€“ solve ìš”ì²­ ì•ˆ í•¨`;
        ocrRunning = false;
        return;
      }

      const englishText = extractEnglishText(rawText);
      const effectiveLen = englishText.replace(/\s/g, "").length;

      // ê·¸ë¦¼/ë„í‘œ ë“±: ì˜ì–´ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ë¬´ì‹œ
      if (!englishText || effectiveLen < 20) {
        statusLine.textContent =
          `OCR ì™„ë£Œ (ì˜ì–´ í…ìŠ¤íŠ¸ ê±°ì˜ ì—†ìŒ: ê¸¸ì´=${effectiveLen}, conf=${tConf.toFixed(1)}%) â€“ ë¬´ì‹œ`;
        ocrRunning = false;
        return;
      }

      statusLine.textContent =
        `OCR ì™„ë£Œ | ì˜ì–´ ê¸¸ì´=${effectiveLen}, conf=${tConf.toFixed(1)}% | ëª¨ë“œ=${currentMode}${zoomSupport ? " | ì„¼ì„œ ì¤Œ" : ""}`;

      await db.ref("p2p/state").update({
        ocrText: englishText,
        ocrConfidence: tConf,
        cameraBlocked: false,
        ocrUpdatedAt: Date.now()
      });

      const snippet = englishText.slice(0, 400);
      if (snippet !== lastSolvedSnippet) {
        lastSolvedSnippet = snippet;
        await db.ref("p2p/command").set({
          type: "solve",
          timestamp: Date.now()
        });
      }
    } catch (e) {
      console.error(e);
      statusLine.textContent = "OCR ì—ëŸ¬: " + e.toString();
    } finally {
      ocrRunning = false;
    }
  }

  // ë¹„ë””ì˜¤ í”„ë ˆì„ì—ì„œ ì¤‘ì•™ 85% ìº¡ì²˜ + ì „ì²˜ë¦¬ (í´ë°±)
  function fillCanvasFromVideoCenter(canvas, ctx) {
    if (!video.videoWidth || !video.videoHeight) {
      canvas.width = 640;
      canvas.height = 360;
      ctx.fillStyle = "black";
      ctx.fillRect(0, 0, canvas.width, canvas.height);
      return;
    }
    const fullW = video.videoWidth;
    const fullH = video.videoHeight;

    const scaleCrop = 0.85;
    const cropW = Math.floor(fullW * scaleCrop);
    const cropH = Math.floor(fullH * scaleCrop);
    const cropX = Math.floor((fullW - cropW) / 2);
    const cropY = Math.floor((fullH - cropH) / 2);

    const maxW = 2000;
    const scale = Math.min(1, maxW / cropW);
    const outW = Math.floor(cropW * scale);
    const outH = Math.floor(cropH * scale);

    canvas.width = outW;
    canvas.height = outH;
    ctx.drawImage(
      video,
      cropX, cropY, cropW, cropH,
      0, 0, outW, outH
    );

    preprocessCanvasForText(canvas, ctx);
  }

  startCamera();
</script>
</body>
</html>


