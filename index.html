<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>answer-site | Camera</title>
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

  <!-- Firebase -->
  <script src="https://www.gstatic.com/firebasejs/11.1.0/firebase-app-compat.js"></script>
  <script src="https://www.gstatic.com/firebasejs/11.1.0/firebase-database-compat.js"></script>

  <!-- Tesseract OCR -->
  <script src="https://unpkg.com/tesseract.js@4/dist/tesseract.min.js"></script>

  <style>
    * { box-sizing: border-box; }
    body {
      margin: 0;
      padding: 6px;
      background: #000;
      color: #fff;
      font-family: system-ui, -apple-system, BlinkMacSystemFont, sans-serif;
    }
    .wrap {
      width: 100%;
    }

    #videoContainer {
      position: relative;
      width: 100%;
      max-height: calc(100vh - 120px);
      overflow: hidden;
      background: #000;
      border-radius: 8px;
    }
    #video {
      width: 100%;
      height: auto;
      transform-origin: center center;
    }
    #overlayFrame {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      border: 2px solid rgba(0, 255, 0, 0.4);
      border-radius: 6px;
      pointer-events: none;
    }

    #controls {
      margin-top: 6px;
      font-size: 11px;
    }

    #statusLine {
      margin-bottom: 4px;
      color: #c4f1ff;
      white-space: nowrap;
      overflow: hidden;
      text-overflow: ellipsis;
    }

    .row {
      display: flex;
      align-items: center;
      gap: 6px;
    }
    .label {
      font-size: 11px;
      color: #aaa;
      white-space: nowrap;
    }
    #zoomRange {
      flex: 1 1 auto;
    }
    #btnOCRNow {
      flex: 0 0 auto;
      padding: 4px 8px;
      font-size: 11px;
      border-radius: 6px;
      border: 1px solid #666;
      background: #111;
      color: #f5f5f5;
      cursor: pointer;
    }
    #btnOCRNow:active {
      transform: scale(0.97);
    }
  </style>
</head>
<body>
<div class="wrap">
  <div id="videoContainer">
    <video id="video" autoplay playsinline></video>
    <div id="overlayFrame"></div>
  </div>

  <div id="controls">
    <div id="statusLine">ì¹´ë©”ë¼ ì¤€ë¹„ ì¤‘...</div>

    <div class="row">
      <span class="label">ë°°ìœ¨</span>
      <input id="zoomRange" type="range" min="1" max="3" step="0.05" value="1">
      <button id="btnOCRNow">ì§€ê¸ˆ ì¸ì‹</button>
    </div>
  </div>
</div>

<script>
  // ---------------- Firebase ----------------
  const firebaseConfig = {
    apiKey: "AIzaSyAvjpHfmbuHQq3ZeV6mNKQFI9LsnX-vf68",
    authDomain: "answer-site-p2p.firebaseapp.com",
    databaseURL: "https://answer-site-p2p-default-rtdb.asia-southeast1.firebasedatabase.app",
    projectId: "answer-site-p2p",
    storageBucket: "answer-site-p2p.firebasestorage.app",
    messagingSenderId: "364227113735",
    appId: "1:364227113735:web:b18355cf0b16454663cba2",
    measurementId: "G-C8MRHK5ZGS"
  };
  firebase.initializeApp(firebaseConfig);
  const db = firebase.database();

  const video      = document.getElementById('video');
  const statusLine = document.getElementById('statusLine');
  const zoomRange  = document.getElementById('zoomRange');
  const btnOCRNow  = document.getElementById('btnOCRNow');

  // ì˜ì–´ë§Œ ì¸ì‹
  const OCR_LANG = "eng";

  // remote.htmlì—ì„œ ì„ íƒí•œ ëª¨ë“œ í‘œì‹œë§Œ
  let currentMode = "reading";
  db.ref("p2p/state/modeHint").on("value", snap => {
    const m = snap.val();
    if (m) {
      currentMode = m;
      if (statusLine.textContent.startsWith("ì¹´ë©”ë¼ ON")) {
        // í•´ìƒë„ ë¶€ë¶„ì€ ìœ ì§€
        const match = statusLine.textContent.match(/\(.*?\)/);
        const resInfo = match ? ` ${match[0]} ` : " ";
        statusLine.textContent =
          `ì¹´ë©”ë¼ ON${resInfo}â€“ ëª¨ë“œ: ${currentMode} (í™”ë©´ ì•ˆì •ë˜ë©´ ìë™ ì¸ì‹)`;
      }
    }
  });

  let ocrRunning = false;
  let lastSolvedSnippet = "";
  let lastBlocked = null;
  let lastOcrTime = 0;

  // ë°ê¸° + ì›€ì§ì„ ê°ì§€ìš© ì¸ë„¤ì¼ ìº”ë²„ìŠ¤
  const BW = 64, BH = 36;
  const bCanvas = document.createElement('canvas');
  const bCtx = bCanvas.getContext('2d');
  bCanvas.width = BW;
  bCanvas.height = BH;
  let prevThumb = null;
  let lastStableTime = 0;

  // ê³ í•´ìƒë„ ìŠ¤í‹¸ í”„ë ˆì„ìš© ImageCapture (iOSì—ì„œëŠ” ëŒ€ë¶€ë¶„ ë¯¸ì§€ì›, ë‹¤ë¥¸ ê¸°ê¸° fallbackìš©)
  let imageCapture = null;

  zoomRange.addEventListener("input", () => {
    const scale = parseFloat(zoomRange.value);
    video.style.transform = `scale(${scale})`;
  });

  btnOCRNow.addEventListener("click", () => {
    if (ocrRunning) return;
    lastOcrTime = Date.now();
    doOCROnce(true);
  });

  // ---------- ì¹´ë©”ë¼ ì‹œì‘ (iPhone 14 ê¸°ì¤€ íŠœë‹, ë‹¤ë¥¸ ê¸°ê¸°ì—ì„œë„ ë™ì‘) ----------
  async function startCamera() {
    try {
      const constraints = {
        video: {
          facingMode: { ideal: "environment" },
          width: { ideal: 1920, max: 1920 },
          height: { ideal: 1080, max: 1080 },
          frameRate: { ideal: 30, max: 30 }
        },
        audio: false
      };

      const stream = await navigator.mediaDevices.getUserMedia(constraints);
      video.srcObject = stream;

      const track = stream.getVideoTracks()[0];
      if (track) {
        // í…ìŠ¤íŠ¸ì— ìœ ë¦¬í•œ ë””í…Œì¼ ëª¨ë“œ ìš”ì²­
        if ("contentHint" in track) {
          try {
            track.contentHint = "detail";
          } catch (e) {
            console.warn("contentHint ì„¤ì • ì‹¤íŒ¨:", e);
          }
        }

        // ì‹¤ì œ í•´ìƒë„ í‘œì‹œ
        const s = track.getSettings ? track.getSettings() : {};
        const resInfo =
          s.width && s.height ? ` (${s.width}x${s.height}) ` : " ";

        statusLine.textContent =
          `ì¹´ë©”ë¼ ON${resInfo}â€“ ëª¨ë“œ: ${currentMode} (í™”ë©´ ì•ˆì •ë˜ë©´ ìë™ ì¸ì‹)`;

        // ê³ í•´ìƒë„ ìŠ¤í‹¸ ìº¡ì²˜ ì¤€ë¹„ (iOSëŠ” ë³´í†µ ë¯¸ì§€ì›, ë°ìŠ¤í¬íƒ‘/ì•ˆë“œì—ì„œë§Œ ë™ì‘)
        if ("ImageCapture" in window) {
          try {
            imageCapture = new ImageCapture(track);
          } catch (e) {
            console.warn("ImageCapture ìƒì„± ì‹¤íŒ¨, video í”„ë ˆì„ fallback:", e);
            imageCapture = null;
          }
        }
      }

      startAutoLoop();
    } catch (e) {
      console.error(e);
      statusLine.textContent = "ì¹´ë©”ë¼ ì ‘ê·¼ ì‹¤íŒ¨: " + e.toString();
    }
  }

  // ---------- ë°ê¸° + ì›€ì§ì„ ë¶„ì„ ----------
  function analyzeFrame() {
    if (!video.videoWidth || !video.videoHeight) {
      return { blocked: false, stable: false };
    }

    bCtx.drawImage(video, 0, 0, BW, BH);
    const data = bCtx.getImageData(0, 0, BW, BH).data;
    const len = BW * BH;

    let sumBright = 0;
    let diffSum = 0;
    const currThumb = new Uint8Array(len);

    for (let i = 0, p = 0; i < data.length; i += 4, p++) {
      const r = data[i];
      const g = data[i + 1];
      const b = data[i + 2];
      const gray = (r + g + b) / 3;
      currThumb[p] = gray;
      sumBright += gray;
      if (prevThumb) {
        diffSum += Math.abs(gray - prevThumb[p]);
      }
    }

    const avgBright = sumBright / len;
    const diffAvg   = prevThumb ? (diffSum / len) : 0;

    // ë„ˆë¬´ ì–´ë‘ìš´ ê²½ìš°ë§Œ "ê°€ë ¤ì§"ìœ¼ë¡œ ê°„ì£¼ (ì„ê³„ê°’ ì™„í™”)
    const blocked = avgBright < 15;

    const now = Date.now();
    let stable = false;
    if (diffAvg < 8) {
      if (!lastStableTime) {
        lastStableTime = now;
      } else if (now - lastStableTime > 1000) {
        stable = true;
      }
    } else {
      lastStableTime = 0;
    }

    prevThumb = currThumb;

    return { blocked, stable };
  }

  // ---------- ìë™ ë£¨í”„ ----------
  function startAutoLoop() {
    function loop() {
      const now = Date.now();

      const { blocked } = analyzeFrame();

      if (blocked !== lastBlocked) {
        lastBlocked = blocked;
        db.ref("p2p/state").update({
          cameraBlocked: blocked,
          cameraUpdatedAt: Date.now()
        });
        const base = statusLine.textContent.replace(/ Â· ì¹´ë©”ë¼ ìƒíƒœ:.+$/, "");
        statusLine.textContent = blocked
          ? `${base} Â· ì¹´ë©”ë¼ ìƒíƒœ: ê°€ë ¤ì§`
          : `${base} Â· ì¹´ë©”ë¼ ìƒíƒœ: ì •ìƒ`;
      }

      // âœ… 8ì´ˆë§ˆë‹¤ ìë™ OCR: í™”ë©´ì´ ì•ˆ ê°€ë ¤ì ¸ ìˆê³ , í˜„ì¬ OCR ì¤‘ì´ ì•„ë‹ ë•Œ
      if (!blocked && !ocrRunning && now - lastOcrTime > 8000) {
        lastOcrTime = now;
        doOCROnce(false);
      }

      requestAnimationFrame(loop);
    }
    requestAnimationFrame(loop);
  }

  // ---------- ìŠ¤í‹¸ ìº¡ì²˜ ----------
  async function captureToCanvas(canvas, ctx) {
    // 1ìˆœìœ„: takePhoto()
    if (imageCapture && typeof imageCapture.takePhoto === "function") {
      try {
        const blob = await imageCapture.takePhoto();
        const imgBitmap = await createImageBitmap(blob);
        drawCroppedScaled(imgBitmap, canvas, ctx);
        return;
      } catch (e) {
        console.warn("takePhoto ì‹¤íŒ¨, grabFrame ì‹œë„:", e);
      }
    }

    // 2ìˆœìœ„: grabFrame()
    if (imageCapture && typeof imageCapture.grabFrame === "function") {
      try {
        const bitmap = await imageCapture.grabFrame();
        drawCroppedScaled(bitmap, canvas, ctx);
        return;
      } catch (e) {
        console.warn("grabFrame ì‹¤íŒ¨, video í”„ë ˆì„ fallback:", e);
      }
    }

    // 3ìˆœìœ„: video í”„ë ˆì„
    fillCanvasFromVideoCenter(canvas, ctx);
  }

  // ì¤‘ì•™ 90% í¬ë¡­ + ê°€ë¡œ ìµœëŒ€ 1600px (imageBitmap/bitmapìš©)
  function drawCroppedScaled(source, canvas, ctx) {
    const fullW = source.width;
    const fullH = source.height;

    const scaleCrop = 0.9;
    const cropW = Math.floor(fullW * scaleCrop);
    const cropH = Math.floor(fullH * scaleCrop);
    const cropX = Math.floor((fullW - cropW) / 2);
    const cropY = Math.floor((fullH - cropH) / 2);

    const maxW = 1600;
    const scale = Math.min(1, maxW / cropW);
    const outW = Math.floor(cropW * scale);
    const outH = Math.floor(cropH * scale);

    canvas.width = outW;
    canvas.height = outH;

    ctx.imageSmoothingEnabled = false;
    ctx.imageSmoothingQuality = "high";

    ctx.drawImage(
      source,
      cropX, cropY, cropW, cropH,
      0, 0, outW, outH
    );
  }

  // ë¹„ë””ì˜¤ í”„ë ˆì„ì—ì„œ ì¤‘ì•™ 90% ìº¡ì²˜ (iOS ë“± í´ë°±)
  function fillCanvasFromVideoCenter(canvas, ctx) {
    if (!video.videoWidth || !video.videoHeight) {
      canvas.width = 640;
      canvas.height = 360;
      ctx.fillStyle = "black";
      ctx.fillRect(0, 0, canvas.width, canvas.height);
      return;
    }
    const fullW = video.videoWidth;
    const fullH = video.videoHeight;

    const scaleCrop = 0.9;
    const cropW = Math.floor(fullW * scaleCrop);
    const cropH = Math.floor(fullH * scaleCrop);
    const cropX = Math.floor((fullW - cropW) / 2);
    const cropY = Math.floor((fullH - cropH) / 2);

    const maxW = 1600;
    const scale = Math.min(1, maxW / cropW);
    const outW = Math.floor(cropW * scale);
    const outH = Math.floor(cropH * scale);

    canvas.width = outW;
    canvas.height = outH;

    ctx.imageSmoothingEnabled = false;
    ctx.imageSmoothingQuality = "high";

    ctx.drawImage(
      video,
      cropX, cropY, cropW, cropH,
      0, 0, outW, outH
    );
  }

  // ---------- OCR 1íšŒ ----------
  async function doOCROnce(manual) {
    if (ocrRunning) return;
    ocrRunning = true;

    try {
      statusLine.textContent = manual
        ? "ìˆ˜ë™ OCR ì§„í–‰ ì¤‘..."
        : "ìë™ OCR ì§„í–‰ ì¤‘...";

      const canvas = document.createElement('canvas');
      const ctx = canvas.getContext('2d');

      await captureToCanvas(canvas, ctx);

      const result = await Tesseract.recognize(canvas, OCR_LANG, {
        logger: m => {
          if (m.status === "recognizing text") {
            statusLine.textContent =
              (manual ? "[ìˆ˜ë™] " : "[ìë™] ") +
              `OCR ì§„í–‰ ì¤‘... ${Math.round((m.progress || 0) * 100)}%`;
          }
        },
        tessedit_pageseg_mode: 6
      });

      const rawText = result.data.text || "";
      const text = rawText.replace(/\s+\n/g, "\n").trim();
      const conf = result.data.confidence || 0;

      const effectiveLen = text.replace(/\s/g, "").length;

      // ğŸ”µ í’ˆì§ˆê³¼ ìƒê´€ì—†ì´ OCR ê²°ê³¼ëŠ” í•­ìƒ Firebaseì— ì˜¬ë¦¼ (remote ë””ë²„ê·¸ìš©)
      await db.ref("p2p/state").update({
        ocrText: text,
        ocrConfidence: conf,
        cameraBlocked: false,
        ocrUpdatedAt: Date.now()
      });

      // í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ AI í˜¸ì¶œì€ ì•ˆ í•¨
      if (!text || effectiveLen < 10) {
        statusLine.textContent =
          `OCR ì™„ë£Œ (í…ìŠ¤íŠ¸ ë„ˆë¬´ ì§§ìŒ: ê¸¸ì´=${effectiveLen}, conf=${conf.toFixed(1)}%)`;
        ocrRunning = false;
        return;
      }

      // ì‹ ë¢°ë„ 10% ë¯¸ë§Œì´ë©´ AI í˜¸ì¶œì€ ì•ˆ í•¨
      if (conf < 10) {
        statusLine.textContent =
          `OCR ì™„ë£Œ (ì‹ ë¢°ë„ ${conf.toFixed(1)}%ë¡œ ë‚®ì•„ì„œ í’€ì´ íŠ¸ë¦¬ê±°ëŠ” ìƒëµí•¨)`;
        ocrRunning = false;
        return;
      }

      statusLine.textContent =
        `OCR ì™„ë£Œ | ê¸¸ì´=${effectiveLen}, conf=${conf.toFixed(1)}% | ëª¨ë“œ=${currentMode}`;

      // ğŸ”µ ì¶©ë¶„íˆ ê¸¸ê³  confë„ 10% ì´ìƒì¼ ë•Œë§Œ solve íŠ¸ë¦¬ê±°
      const snippet = text.slice(0, 400);
      if (snippet !== lastSolvedSnippet) {
        lastSolvedSnippet = snippet;
        await db.ref("p2p/command").set({
          type: "solve",
          timestamp: Date.now()
        });
      }
    } catch (e) {
      console.error(e);
      statusLine.textContent = "OCR ì—ëŸ¬: " + e.toString();
    } finally {
      ocrRunning = false;
    }
  }

  startCamera();
</script>
</body>
</html>

